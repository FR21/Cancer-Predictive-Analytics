# -*- coding: utf-8 -*-
"""Cancer_Predictive Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10XUYgBqulWrHZORxEqEcIy59Nc8msykf

# **Global Cancer Severity Predictor: Leveraging Machine Learning for Early Risk Assessment**
- **Name:** Felix Rafael
- **Email:** felixrafaelkwan@gmail.com


### **Objective:**   
The objective of this project is to develop a robust machine learning regression model to accurately predict the severity score of cancer patients based on lifestyle, genetic, and environmental factors. By utilizing advanced regression techniques, the model aims to assist healthcare professionals and researchers in assessing cancer severity at an early stage, supporting proactive decision-making and tailored treatment strategies.

###**Dataset source:**
https://www.kaggle.com/datasets/zahidmughal2343/global-cancer-patients-2015-2024

# **1. Import Libraries**
"""

# Data manipulation and analysis
import numpy as np
import pandas as pd
# Data visualization
import matplotlib.pyplot as plt
import seaborn as sns
# Data preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
# Regression models
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
# Model evaluation metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
# Hyperparameter tuning
from sklearn.model_selection import RandomizedSearchCV

"""# **2. Data Loading**"""

# Read CSV files - Cancer Patients
df = pd.read_csv("cancer_patients.csv")

# Show the first 5 rows of Cancer Patients Datasets
df.head()

"""# **3. Exploratory Data Analysis (EDA)**"""

# Review the number of rows and columns in the dataset
print(f"Number of rows: {df.shape[0]}")
print(f"Number of columns: {df.shape[1]}")

# Check columns and data types
df.info()

# Show overall descriptive statistics
df.describe(include="all")

# Check the number of missing values
df.isna().sum()

# Check the number of duplicate rows
df.duplicated().sum()

# Check the number of duplicate columns
df.columns.duplicated()

# Define column types
categorical_columns = ['Gender', 'Country_Region', 'Cancer_Type', 'Cancer_Stage']
numeric_columns = df.select_dtypes(include=['number']).columns

# Shows the distribution for each numeric column
fig, axes = plt.subplots(nrows=len(numeric_columns), ncols=1, figsize=(12, 4 * len(numeric_columns)))
for i, col in enumerate(numeric_columns):
    sns.histplot(df[col], bins=30, kde=True, ax=axes[i])
    axes[i].set_title(f'Distribution of {col}', fontsize=14)
plt.tight_layout()
plt.show()

# Custom order for Cancer_Stage
custom_order = {
    'Cancer_Stage': ['Stage 0', 'Stage I', 'Stage II', 'Stage III', 'Stage IV']
}

# Plot the percentage distribution of each categorical column
for col in categorical_columns:
    plt.figure(figsize=(12, 6))
    order = custom_order.get(col, df[col].value_counts().index)
    total = len(df)

    ax = sns.countplot(
        data=df,
        x=col,
        order=order,
        edgecolor="black",
        hue=df[col],
        palette="Set2",
        legend=False
    )

    for p in ax.patches:
        height = p.get_height()
        percent = f'{100 * height / total:.1f}%'
        ax.annotate(percent,
                    (p.get_x() + p.get_width() / 2., height / 2),
                    ha='center', va='center',
                    fontsize=10, color='white', fontweight='bold')

    plt.title(f'Percentage Distribution of {col}', fontsize=14, fontweight='bold')
    plt.xlabel(col, fontsize=12)
    plt.ylabel("Count", fontsize=12)
    plt.xticks(rotation=45)
    plt.grid(axis='y', linestyle='--', linewidth=0.5, alpha=0.7)
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.2)
    plt.show()

# Calculate correlation between numeric features
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix")
plt.show()

# Shows a visualization of the relationship between numeric features
sns.pairplot(df, diag_kind="kde")
plt.show()

"""# **4. Data Preprocessing**"""

# Drop unnecessary columns: Patient_ID, Gender, Country_Region, and Year
df_cleaned = df.drop(columns=['Patient_ID', 'Gender', 'Country_Region', 'Year'])
df_cleaned.head()

# Define list of numeric feature columns for analysis
numeric_cols = ['Age', 'Genetic_Risk', 'Air_Pollution', 'Alcohol_Use',
                'Smoking', 'Obesity_Level', 'Treatment_Cost_USD', 'Survival_Years']

# Create boxplot for each numeric column to visualize distribution and detect outliers
sns.set(style="whitegrid")
for col in numeric_cols:
    plt.figure(figsize=(10, 4))
    ax = sns.boxplot(
        x=df[col],
        color="skyblue",
        width=0.5,
        fliersize=5,
        linewidth=1.5,
        boxprops=dict(facecolor='lightblue', edgecolor='black'),
        medianprops=dict(color='red', linewidth=2),
        whiskerprops=dict(color='black'),
        capprops=dict(color='black'),
        flierprops=dict(marker='o', markerfacecolor='orange', markersize=5, linestyle='none')
    )

    plt.title(f'Distribution and Outliers: {col}', fontsize=14, fontweight='bold')
    plt.xlabel(col, fontsize=12)
    plt.ylabel("Value Distribution", fontsize=11)
    plt.grid(axis='x', linestyle='--', linewidth=0.5, alpha=0.7)
    plt.tight_layout()
    plt.show()

# Select only the specified numeric columns
df_numeric = df[numeric_cols]

# Calculate the first quartile (Q1), third quartile (Q3), and interquartile range (IQR)
Q1 = df_numeric.quantile(0.25)
Q3 = df_numeric.quantile(0.75)
IQR = Q3 - Q1

# Calculate lower and upper bounds for detecting outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Count the number of outliers in each numeric column
outlier_counts = ((df_numeric < lower_bound) | (df_numeric > upper_bound)).sum().sort_values(ascending=False)
print("Number of Outliers per Column:\n")
print(outlier_counts)

# Apply standardization only to numeric features
scaler = StandardScaler()
df_cleaned[numeric_cols] = scaler.fit_transform(df_cleaned[numeric_cols])
df_cleaned.head()

# Select only numeric features as model inputs
df_numeric = df_cleaned[numeric_cols]

# Plot boxplot of numeric features after standardization
plt.figure(figsize=(12, 6))
sns.boxplot(data=df_numeric, orient="v", palette="Set3", width=0.6, fliersize=4, linewidth=1)
plt.title("Boxplot of Numeric Features (Predictors) After Standardization", fontsize=16, fontweight='bold')
plt.xlabel("Features", fontsize=12)
plt.ylabel("Standardized Values", fontsize=12)
plt.xticks(rotation=30)
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# Encode categorical columns 'Cancer_Type' and 'Cancer_Stage' using one-hot encoding
df_encoded = pd.get_dummies(df_cleaned, columns=['Cancer_Type', 'Cancer_Stage'], dtype=int)
df_encoded.head()

# Separate features and target variable
X = df_encoded.drop(columns=['Target_Severity_Score'])
y = df_encoded['Target_Severity_Score']

# Split data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""# **5. Model Development**

## **a. Random Forest**
"""

# Initialize the model
rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
# Training model
rf_model.fit(X_train, y_train)

"""## **b. XGBoost**"""

# Initialize the model
xgb_model = XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=6, random_state=42)
# Training model
xgb_model.fit(X_train, y_train)

"""## **c. LightGBM**"""

# Initialize the model
lgb_model = LGBMRegressor(n_estimators=200, learning_rate=0.1, max_depth=6, random_state=42)
# Training model
lgb_model.fit(X_train, y_train)

"""# **6. Model Evaluation**

## **a. Random Forest**
"""

# Predict on training data using the trained Random Forest model
y_train_pred_rf = rf_model.predict(X_train)

# Evaluate performance on training data
mae_train_rf = mean_absolute_error(y_train, y_train_pred_rf)
rmse_train_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))
r2_train_rf = r2_score(y_train, y_train_pred_rf)

print("Random Forest Evaluation on Training Set:")
print(f"MAE: {mae_train_rf:.4f}")
print(f"RMSE: {rmse_train_rf:.4f}")
print(f"R2 Score: {r2_train_rf:.4f}")

# Predict on test data using the trained Random Forest model
y_pred_rf = rf_model.predict(X_test)

# Evaluate model performance on test data
mae_rf = mean_absolute_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

print("Random Forest Regressor:")
print(f"MAE: {mae_rf:.4f}")
print(f"RMSE: {rmse_rf:.4f}")
print(f"R2 Score: {r2_rf:.4f}")

# Plot scatterplot of actual vs predicted values for Random Forest model
plt.figure(figsize=(6, 6))
sns.scatterplot(x=y_test, y=y_pred_rf, color='skyblue', edgecolor='black', alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel('Actual Severity Score')
plt.ylabel('Predicted Severity Score')
plt.title('Random Forest - Actual vs Predicted')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Calculate residuals for Random Forest predictions
residuals_rf = y_test - y_pred_rf

# Plot residuals versus predicted values to assess model errors
plt.figure(figsize=(8, 4))
sns.scatterplot(x=y_pred_rf, y=residuals_rf, color='orange', edgecolor='black', alpha=0.5)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Predicted Severity Score')
plt.ylabel('Residuals')
plt.title('Random Forest - Residual Plot')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Plot distribution of residuals for Random Forest predictions with KDE curve
plt.figure(figsize=(6, 4))
sns.histplot(residuals_rf, kde=True, color='purple', edgecolor='black', bins=30)
plt.title('Random Forest - Residuals Distribution')
plt.xlabel('Residual')
plt.ylabel('Frequency')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

"""## **b. XGBoost**"""

# Predict on training data using the trained XGBoost model
y_train_pred_xgb = xgb_model.predict(X_train)

# Evaluate performance on training data
mae_train_xgb = mean_absolute_error(y_train, y_train_pred_xgb)
rmse_train_xgb = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))
r2_train_xgb = r2_score(y_train, y_train_pred_xgb)

print("XGBoost Regressor Evaluation on Training Set:")
print(f"MAE: {mae_train_xgb:.4f}")
print(f"RMSE: {rmse_train_xgb:.4f}")
print(f"R2 Score: {r2_train_xgb:.4f}")

# Predict on test data using the trained XGBoost model
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate model performance on test data
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
r2_xgb = r2_score(y_test, y_pred_xgb)

print("XGBoost Regressor:")
print(f"MAE: {mae_xgb:.4f}")
print(f"RMSE: {rmse_xgb:.4f}")
print(f"R2 Score: {r2_xgb:.4f}")

# Plot scatterplot of actual vs predicted values for XGBoost model
plt.figure(figsize=(6, 6))
sns.scatterplot(x=y_test, y=y_pred_xgb, color='skyblue', edgecolor='black', alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel('Actual Severity Score')
plt.ylabel('Predicted Severity Score')
plt.title('XGBoost - Actual vs Predicted')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Calculate residuals for XGBoost predictions
residuals_xgb = y_test - y_pred_xgb

# Plot residuals versus predicted values to assess model errors
plt.figure(figsize=(8, 4))
sns.scatterplot(x=y_pred_xgb, y=residuals_xgb, color='orange', edgecolor='black', alpha=0.5)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Predicted Severity Score')
plt.ylabel('Residuals')
plt.title('XGBoost - Residual Plot')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Plot distribution of residuals for XGBoost predictions with KDE curve
plt.figure(figsize=(6, 4))
sns.histplot(residuals_xgb, kde=True, color='purple', edgecolor='black', bins=30)
plt.title('XGBoost - Residuals Distribution')
plt.xlabel('Residual')
plt.ylabel('Frequency')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

"""## **c. LightGBM**"""

# Predict on training data using the trained LightGBM model
y_train_pred_lgb = lgb_model.predict(X_train)

# Evaluate model performance on training data
mae_train_lgb = mean_absolute_error(y_train, y_train_pred_lgb)
rmse_train_lgb = np.sqrt(mean_squared_error(y_train, y_train_pred_lgb))
r2_train_lgb = r2_score(y_train, y_train_pred_lgb)

print("LightGBM Regressor Evaluation on Training Set:")
print(f"MAE: {mae_train_lgb:.4f}")
print(f"RMSE: {rmse_train_lgb:.4f}")
print(f"R2 Score: {r2_train_lgb:.4f}")

# Predict on test data using the trained LightGBM model
y_pred_lgb = lgb_model.predict(X_test)

# Evaluate model performance on test data
mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))
r2_lgb = r2_score(y_test, y_pred_lgb)

print("LightGBM Regressor:")
print(f"MAE: {mae_lgb:.4f}")
print(f"RMSE: {rmse_lgb:.4f}")
print(f"R2 Score: {r2_lgb:.4f}")

# Plot Actual vs Predicted values for LightGBM model
plt.figure(figsize=(6, 6))
sns.scatterplot(x=y_test, y=y_pred_lgb, color='skyblue', edgecolor='black', alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel('Actual Severity Score')
plt.ylabel('Predicted Severity Score')
plt.title('LightGBM - Actual vs Predicted')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Calculate residuals for LightGBM predictions
residuals_lgb = y_test - y_pred_lgb

# Plot residuals versus predicted values to assess model errors
plt.figure(figsize=(8, 4))
sns.scatterplot(x=y_pred_lgb, y=residuals_lgb, color='orange', edgecolor='black', alpha=0.5)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Predicted Severity Score')
plt.ylabel('Residuals')
plt.title('LightGBM - Residual Plot')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Plot distribution of residuals for LightGBM predictions with KDE curve
plt.figure(figsize=(6, 4))
sns.histplot(residuals_lgb, kde=True, color='purple', edgecolor='black', bins=30)
plt.title('LightGBM - Residuals Distribution')
plt.xlabel('Residual')
plt.ylabel('Frequency')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

"""# **7. Hyperparameter Tunning**

## **a. XGBoost**
"""

# Define the hyperparameter grid for XGBoost
xgb_param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7, 10],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'reg_alpha': [0, 0.01, 0.1],  # L1 regularization term
    'reg_lambda': [1, 1.5, 2.0]   # L2 regularization term
}

# Initialize the XGBoost regressor with a fixed random state for reproducibility
xgb = XGBRegressor(random_state=42)

# Setup RandomizedSearchCV for hyperparameter tuning
xgb_random = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=xgb_param_grid,
    n_iter=50,            # Number of parameter settings sampled
    cv=3,                 # 3-fold cross-validation
    verbose=1,            # Show progress messages
    random_state=42,      # For reproducibility
    scoring='neg_mean_squared_error',  # Evaluation metric to minimize
    n_jobs=-1             # Use all available CPU cores
)

# Fit the randomized search model on training data
xgb_random.fit(X_train, y_train)

# Display the best hyperparameters found
print("Best Parameters for XGBoost:")
print(xgb_random.best_params_)

# Predict on test data using the best model
y_pred_xgb_tuned = xgb_random.predict(X_test)

# Evaluate the tuned model's performance
print("Evaluation After Tuning:")
print(f"MAE: {mean_absolute_error(y_test, y_pred_xgb_tuned):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_xgb_tuned)):.4f}")
print(f"R2: {r2_score(y_test, y_pred_xgb_tuned):.4f}")

# Plot Actual vs Predicted values for XGBoost model after Hyperparameter Tunning
def plot_actual_vs_predicted(y_test, y_pred, title='Actual vs Predicted'):
    plt.figure(figsize=(7, 6))
    sns.scatterplot(x=y_test, y=y_pred, alpha=0.6, color='royalblue', edgecolor='white')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
    plt.xlabel('Actual Severity Score')
    plt.ylabel('Predicted Severity Score')
    plt.title(title)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()
plot_actual_vs_predicted(y_test, y_pred_xgb_tuned , title='XGBoost: Actual vs Predicted')

# Calculate residuals from the tuned XGBoost model after Hyperparameter Tunning
residuals_lgb_tuned = y_test - y_pred_xgb_tuned
plt.figure(figsize=(8, 4))
sns.scatterplot(
    x=y_pred_xgb_tuned,
    y=residuals_lgb_tuned,
    color='mediumseagreen',
    edgecolor='black',
    alpha=0.6
)
plt.axhline(0, color='red', linestyle='--', linewidth=1.2)
plt.xlabel('Predicted Severity Score', fontsize=11)
plt.ylabel('Residuals', fontsize=11)
plt.title('XGBoost (Tuned) - Residual Plot', fontsize=13, fontweight='bold')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Plot residual distribution for tuned XGBoost model
def plot_residuals(y_test, y_pred, title='Residual Plot'):
    residuals = y_test - y_pred
    plt.figure(figsize=(7, 5))
    sns.histplot(residuals, bins=30, kde=True, color='darkorange')
    plt.axvline(0, color='red', linestyle='--')
    plt.title(title)
    plt.xlabel('Residuals')
    plt.ylabel('Frequency')
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()
plot_residuals(y_test, y_pred_xgb_tuned, title='XGBoost: Residual Distribution')

"""## **b. LightGBM**"""

# Define the hyperparameter grid for LightGBM
lgb_param_grid = {
    'n_estimators': [100, 200, 300],            # Number of boosting rounds
    'learning_rate': [0.01, 0.05, 0.1, 0.2],   # Step size shrinkage used in update to prevent overfitting
    'num_leaves': [31, 50, 100],                # Maximum number of leaves in one tree
    'max_depth': [-1, 5, 10],                   # Maximum tree depth (-1 means no limit)
    'min_child_samples': [10, 20, 30],          # Minimum number of data needed in a child (leaf)
    'subsample': [0.6, 0.8, 1.0],               # Fraction of data to be used for each tree (row sampling)
    'colsample_bytree': [0.6, 0.8, 1.0]         # Fraction of features used for each tree (feature sampling)
}

# Initialize the LightGBM Regressor model
lgb = LGBMRegressor(random_state=42)

# Set up randomized search with cross-validation for hyperparameter tuning
lgb_random = RandomizedSearchCV(
    estimator=lgb,
    param_distributions=lgb_param_grid,
    n_iter=50,                             # Number of parameter settings sampled
    cv=3,                                 # 3-fold cross-validation
    verbose=1,                            # Show progress messages
    random_state=42,                      # For reproducibility
    scoring='neg_mean_squared_error',    # Evaluation metric to minimize
    n_jobs=-1                            # Use all available CPU cores
)

# Fit the randomized search model on training data
lgb_random.fit(X_train, y_train)

# Display the best hyperparameters found
print("Best Parameters for LightGBM:")
print(lgb_random.best_params_)

# Predict on test data using the best model
y_pred_lgb_tuned = lgb_random.predict(X_test)

# Evaluate the tuned model's performance
print("Evaluation After Tuning:")
print(f"MAE: {mean_absolute_error(y_test, y_pred_lgb_tuned):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_lgb_tuned)):.4f}")
print(f"R2: {r2_score(y_test, y_pred_lgb_tuned):.4f}")

# Plot Actual vs Predicted values for LightGBM model after Hyperparameter Tunning
def plot_actual_vs_predicted(y_test, y_pred, title='Actual vs Predicted'):
    plt.figure(figsize=(7, 6))
    sns.scatterplot(x=y_test, y=y_pred, alpha=0.6, color='royalblue', edgecolor='white')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')
    plt.xlabel('Actual Severity Score')
    plt.ylabel('Predicted Severity Score')
    plt.title(title)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()
plot_actual_vs_predicted(y_test, y_pred_lgb_tuned, title='LightGBM: Actual vs Predicted')

# Calculate residuals from the tuned LightGBM model after Hyperparameter Tunning
residuals_lgb_tuned = y_test - y_pred_lgb_tuned
plt.figure(figsize=(8, 4))
sns.scatterplot(
    x=y_pred_lgb_tuned,
    y=residuals_lgb_tuned,
    color='mediumseagreen',
    edgecolor='black',
    alpha=0.6
)
plt.axhline(0, color='red', linestyle='--', linewidth=1.2)
plt.xlabel('Predicted Severity Score', fontsize=11)
plt.ylabel('Residuals', fontsize=11)
plt.title('LightGBM (Tuned) - Residual Plot', fontsize=13, fontweight='bold')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Plot residual distribution for tuned LightGBM model
def plot_residuals(y_test, y_pred, title='Residual Plot'):
    residuals = y_test - y_pred
    plt.figure(figsize=(7, 5))
    sns.histplot(residuals, bins=30, kde=True, color='darkorange')
    plt.axvline(0, color='red', linestyle='--')
    plt.title(title)
    plt.xlabel('Residuals')
    plt.ylabel('Frequency')
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()
plot_residuals(y_test, y_pred_lgb_tuned, title='LightGBM: Residual Distribution')